{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 ai사이트\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "import time\n",
    "import pyautogui\n",
    "\n",
    "# 크롬 드라이버 초기화\n",
    "driver = webdriver.Chrome()\n",
    "cultures = [\"문화회관\", \"도서관\", \"박물관\", \"기타\", \"공원장\", \"미술관\", \"문화원\"]\n",
    "\n",
    "\n",
    "addresses = ['중랑구','강남','강동','강북','강서','관악','광진','구로','금천','노원','도봉',\n",
    "             '동대문','동작','마포','서대문','서초','성동','송파','양천','영등포',\n",
    "             '용산','은평','종로']\n",
    "\n",
    "# gangnam 카테고리 초기화\n",
    "# gangnam = {\"문화회관\": 0, \"도서관\": 0, \"박물관\": 0, \"기타\": 0, \"공원장\": 0,\"미술관\": 0, \"문화원\": 0}\n",
    "cultures_dic = {\"전시회\": 0, \"음악 및 무용발표회\": 0, \"박물관\": 0, \"전통예술공연\": 0, \"연극공연\": 0,\"대중공연\": 0, \"문화원\": 0,'운동경기': 0,}\n",
    "# for i in addresses:\n",
    "    \n",
    "# 데이터 읽기\n",
    "data = pd.read_csv('filtered_data_중랑구.csv', encoding='utf-8')\n",
    "# data = pd.read_excel('filtered_data_중랑구.xlsx')\n",
    "urls = data['홈페이지']\n",
    "position = data['문화시설명']\n",
    "\n",
    "# CSV 파일에 헤더(컬럼 이름) 기록\n",
    "with open(f'count_data_중랑구.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # 헤더 작성 (문화시설명 + gangnam의 키 값들)\n",
    "    header = ['문화시설명'] + list(cultures_dic.keys())\n",
    "    writer.writerow(header)\n",
    "\n",
    "# 각 홈페이지를 순회하며 텍스트 분석\n",
    "for homepage, facility_name in zip(urls, position):\n",
    "    cultures_dic = {\"전시회\": 0, \"음악 및 무용발표회\": 0, \"박물관\": 0, \"전통예술공연\": 0, \"연극공연\": 0,\"대중공연\": 0, \"문화원\": 0,'운동경기': 0,}\n",
    "    print(position, end= ' ')\n",
    "    try: \n",
    "        # 셀레니움으로 페이지 요청 및 로드\n",
    "        driver.get(homepage)\n",
    "        WebDriverWait(driver, 10).until(lambda driver: driver.execute_script(\"return document.readyState\") == \"complete\")\n",
    "        try:\n",
    "            close_button = driver.find_element(By.XPATH, '//*[text()=\"닫기\"]')\n",
    "            for i in len(close_button):\n",
    "                time.sleep(0.2)\n",
    "                close_button.click()\n",
    "        except:\n",
    "            print('닫기 버튼 없음')\n",
    "\n",
    "        # 페이지의 body 태그에서 모든 텍스트 가져오기\n",
    "        body_text = driver.find_element(By.TAG_NAME, 'body').text\n",
    "        \n",
    "        body_list_1 = body_text.split('\\t')\n",
    "        body_list_2 = [item for sublist in body_list_1 for item in sublist.split(' ')]\n",
    "        body_list_3 = [item for sublist in body_list_2 for item in sublist.split('\\n')]\n",
    "        body_set = set(body_list_3)\n",
    "        print(len(body_set))\n",
    "        print(position, end= ' ')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    try:\n",
    "        # body_list에서 각 줄을 확인하며 카테고리 찾기\n",
    "        for body in body_set:\n",
    "            driver.get('https://aiopen.etri.re.kr/demo/WordRel')\n",
    "            \n",
    "            try:\n",
    "                word1 = driver.find_element(By.XPATH, '/html/body/div[6]/div[2]/div[2]/table/tbody/tr[1]/td/input')\n",
    "            except:\n",
    "                word1 = driver.find_element(By.ID,'box3')\n",
    "            # time.sleep(1)\n",
    "            try:\n",
    "                word2 = driver.find_element(By.XPATH, '/html/body/div[6]/div[2]/div[2]/table/tbody/tr[2]/td/input')\n",
    "            except:\n",
    "                word2 = driver.find_element(By.ID,'word2')\n",
    "            # time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                button = driver.find_element(By.XPATH, '/html/body/div[6]/div[2]/div[3]/button')\n",
    "            except:\n",
    "                try:\n",
    "                    button = driver.find_element(By.CLASS_NAME,'btnBlue')\n",
    "                except:\n",
    "                    button = driver.find_element(By.CSS_SELECTOR,'btnBlue')\n",
    "\n",
    "            WebDriverWait(driver, 10).until(lambda driver: driver.execute_script(\"return document.readyState\") == \"complete\")\n",
    "            # time.sleep(1)\n",
    "            word2.click()\n",
    "            time.sleep(0.1)\n",
    "            pyautogui.hotkey('ctrl', 'a')\n",
    "            time.sleep(0.2)\n",
    "            pyautogui.hotkey('del')\n",
    "            word2.send_keys(body)\n",
    "            # time.sleep(1)\n",
    "\n",
    "            # 각 장소의 홈페이지에 어떤 키워드가 있는지 분석\n",
    "            for category in cultures_dic.keys():\n",
    "                word1.click()\n",
    "                time.sleep(0.2)\n",
    "                pyautogui.hotkey('ctrl', 'a')\n",
    "                time.sleep(0.2)\n",
    "                pyautogui.hotkey('del')\n",
    "                word1.send_keys(category)\n",
    "                #if category in body:\n",
    "                # time.sleep(0.2)\n",
    "                WebDriverWait(driver, 10).until(lambda driver: driver.execute_script(\"return document.readyState\") == \"complete\")\n",
    "                button.click()\n",
    "                time.sleep(0.1)\n",
    "\n",
    "                try:\n",
    "                    value = float(driver.find_element(By.XPATH, '//*[@id=\"api_result_box_3\"]/table[2]/tbody/tr/td[1]').text)\n",
    "                except:\n",
    "                    value = float(driver.find_element(By.XPATH, '/html/body/div[6]/div[2]/div[5]/table[2]/tbody/tr/td[1]').text)    \n",
    "                try:\n",
    "                    value_2 = float(driver.find_element(By.XPATH, '/html/body/div[6]/div[2]/div[5]/table[1]/tbody/tr/td[2]').text)\n",
    "                except:\n",
    "                    value_2 = float(driver.find_element(By.XPATH, '//*[@id=\"api_result_box_3\"]/table[1]/tbody/tr/td[2]').text)\n",
    "                time.sleep(0.2)                                     \n",
    "                try:\n",
    "                    if value > 0.2:\n",
    "                        cultures_dic[category] += 1\n",
    "                        print(cultures_dic[category],end= ' ')\n",
    "                except:\n",
    "                    if value_2 > 0:\n",
    "                        cultures_dic[category] += 1\n",
    "                        print(cultures_dic[category], end=' ')\n",
    "                time.sleep(0.1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "                # 데이터 CSV 파일에 기록하기\n",
    "    with open(f'count_data_중랑구.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # gangnam 값을 리스트로 변환하고 position과 결합\n",
    "        row = [facility_name] + list(cultures_dic.values())\n",
    "        writer.writerow(row)\n",
    "    \n",
    "# 결과 출력\n",
    "# print)\n",
    "\n",
    "# 크롬 드라이버 종료\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\vvv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vvv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\vvv/nltk_data'\n    - 'c:\\\\Users\\\\vvv\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\vvv\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\vvv\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\vvv\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m text \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mget_text()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 텍스트를 단어 단위로 토큰화\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 관심 있는 카테고리 리스트\u001b[39;00m\n\u001b[0;32m     26\u001b[0m categories \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m전시회\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m박물관\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m음악\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m무용\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m전통예술\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m연극공연\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m영화관람\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m대중공연\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     29\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\vvv\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\vvv\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\vvv\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vvv\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vvv\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\vvv\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\vvv/nltk_data'\n    - 'c:\\\\Users\\\\vvv\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\vvv\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\vvv\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\vvv\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# 영상에 나오는 코드\n",
    "!pip install requests beautifulsoup4 scikit-learn nltk\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# NLTK의 punkt 데이터 다운로드\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 웹 페이지의 URL\n",
    "url = 'https://www.museum.go.kr/site/main/home'\n",
    "\n",
    "# 웹 페이지 요청 및 파싱\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# 페이지에서 텍스트 추출\n",
    "text = soup.get_text()\n",
    "\n",
    "# 텍스트를 단어 단위로 토큰화\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# 관심 있는 카테고리 리스트\n",
    "categories = [\n",
    "    '전시회', '박물관', '음악', '무용', '전통예술', \n",
    "    '연극공연', '영화관람', '대중공연'\n",
    "]\n",
    "\n",
    "# 카테고리와 관련된 텍스트 비율 계산\n",
    "def calculate_category_proportions(text, categories):\n",
    "    category_counts = {category: 0 for category in categories}\n",
    "    total_words = len(words)\n",
    "    \n",
    "    for category in categories:\n",
    "        category_counts[category] = text.lower().count(category)\n",
    "    \n",
    "    proportions = {category: count / total_words for category, count in category_counts.items()}\n",
    "    \n",
    "    return proportions\n",
    "\n",
    "# 카테고리 비율 계산\n",
    "category_proportions = calculate_category_proportions(text, categories)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"카테고리 비율:\")\n",
    "for category, proportion in category_proportions.items():\n",
    "    print(f\"{category}: {proportion:.4f}\")\n",
    "\n",
    "# TF-IDF 벡터화 및 분석\n",
    "def get_tfidf_scores(text, categories):\n",
    "    vectorizer = TfidfVectorizer(vocabulary=categories)\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    scores = {category: X[0, i] for i, category in enumerate(feature_names)}\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# TF-IDF 점수 계산\n",
    "category_scores = get_tfidf_scores(text, categories)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n카테고리 TF-IDF 점수:\")\n",
    "for category, score in category_scores.items():\n",
    "    print(f\"{category}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 분석 모델\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "\n",
    "# spacy 모델 로드\n",
    "nlp = spacy.load('ko_core_news_sm')\n",
    "\n",
    "# 웹 페이지의 URL\n",
    "url = 'https://www.museum.go.kr/site/main/home'\n",
    "\n",
    "# 웹 페이지 요청 및 파싱\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# 페이지에서 텍스트 추출\n",
    "text = soup.get_text()\n",
    "\n",
    "# 문장을 나누기 (spacy 사용)\n",
    "def split_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "sentences = split_sentences(text)\n",
    "\n",
    "# 관심 있는 카테고리 리스트\n",
    "categories = [\n",
    "    '전시회', '박물관', '음악', '무용', '전통예술', \n",
    "    '연극공연', '영화관람', '대중공연'\n",
    "]\n",
    "\n",
    "# TF-IDF 벡터화 및 분석\n",
    "def get_tfidf_scores(sentences, categories):\n",
    "    vectorizer = TfidfVectorizer(vocabulary=categories)\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    scores = {category: [] for category in categories}\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        tfidf_scores = X[i].toarray()[0]\n",
    "        for j, category in enumerate(categories):\n",
    "            score = tfidf_scores[j]\n",
    "            if score > 0:\n",
    "                scores[category].append((i, score, sentences[i]))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# TF-IDF 점수 계산\n",
    "category_scores = get_tfidf_scores(sentences, categories)\n",
    "\n",
    "# 결과 출력\n",
    "for category, score_data in category_scores.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    if score_data:\n",
    "        for index, score, sentence in sorted(score_data, key=lambda x: -x[1]):\n",
    "            print(f\"문장: {sentence}\")\n",
    "            print(f\"점수: {score:.4f}\")\n",
    "    else:\n",
    "        print(\"관련된 문장이 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 분석 모델\n",
    "import urllib3\n",
    "import json\n",
    "# API 요청 URL\n",
    "openApiURL = \"http://aiopen.etri.re.kr:8000/ParaphraseQA\"\n",
    "\n",
    "# ETRI에서 발급받은 Access Key\n",
    "accessKey = \"5d059aa9-b7b7-483a-ba61-e3572b9693e0\"\n",
    "\n",
    "# 분석할 문장들 예시 (전통과 한옥 관련 문장)\n",
    "sentence1 = \"과일\"\n",
    "sentence2 = '사과'\n",
    "\n",
    "# API 요청 데이터 설정\n",
    "requestJson = {\n",
    "    \"argument\": {\n",
    "        \"sentence1\": sentence1,\n",
    "        \"sentence2\": sentence2\n",
    "    }\n",
    "}\n",
    "\n",
    "# HTTP 통신 설정 및 요청\n",
    "http = urllib3.PoolManager()\n",
    "response = http.request(\n",
    "    \"POST\",\n",
    "    openApiURL,\n",
    "    headers={\"Content-Type\": \"application/json; charset=UTF-8\", \"Authorization\": accessKey},\n",
    "    body=json.dumps(requestJson)\n",
    ")\n",
    "\n",
    "# 응답 결과 출력\n",
    "print(\"[Response Code] \" + str(response.status))\n",
    "print(\"[Response Body]\")\n",
    "print(str(response.data, \"utf-8\"))\n",
    "analize = set(str(response.data, \"utf-8\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
